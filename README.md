

# ğŸŒ Real-Time AI Assistant (Web RAG)

A **real-time Retrieval-Augmented Generation (RAG)** AI assistant that answers questions using **live web search**, powered by a **local LLM (Llama 3)** running via **Ollama**.  
No paid APIs. Fully open-source. Dockerized.

---

## ğŸš€ What This Project Does

- Takes a user question
- Searches the web **in real time** (DuckDuckGo)
- Injects search results as context
- Generates an answer using a **local LLM**
- Runs entirely on your machine

This avoids LLM knowledge cutoffs and provides **up-to-date answers**.

---

## ğŸ§  Architecture Overview



User
â†“
Python Assistant (LangChain)
â†“
DuckDuckGo Search â”€â”€â”
â”œâ”€ Context â†’ LLM (Llama 3 via Ollama)
Ollama Server â”€â”€â”€â”€â”€â”€â”˜



---

## ğŸ› ï¸ Tech Stack

- **Python**
- **LangChain**
- **Ollama** (local LLM runtime)
- **Llama 3 (8B)**
- **DuckDuckGo Search**
- **Docker & Docker Compose**

---

## ğŸ“ Project Structure



genai-realtime-web-rag/
â”‚
â”œâ”€â”€ assistant.py          # Main RAG application
â”œâ”€â”€ Dockerfile            # Docker build instructions
â”œâ”€â”€ docker-compose.yml    # Multi-service orchestration
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ .gitignore





## âš™ï¸ Setup Instructions (Local, Without Docker)

### 1ï¸âƒ£ Install Ollama
Download and install from:
ğŸ‘‰ https://ollama.com

Pull the model:

ollama pull llama3:8b

---

### 2ï¸âƒ£ Create Python Environment


python -m venv venv
venv\Scripts\activate   # Windows


Install dependencies:


pip install -r requirements.txt


---

### 3ï¸âƒ£ Run the Assistant


python assistant.py


---

## ğŸ³ Run Using Docker (Recommended)

### 1ï¸âƒ£ Build & Run Everything


docker compose up --build


This will:

* Start Ollama
* Start the RAG assistant
* Connect them automatically

---

### 2ï¸âƒ£ Example Interaction


ğŸŒ Real-Time AI Assistant (Web RAG)
Type 'exit' to quit

You: What are today's trending AI topics?
ğŸ¤– Thinking...
ğŸ¤–: Recent trends include multimodal AI models, open-source LLMs, and AI regulation discussions.


---

## ğŸ§ª Why Docker Compose?

* One command starts **all services**
* Persistent model storage
* Production-style architecture
* Easy to deploy anywhere

---

## âš ï¸ Limitations

* Web search quality depends on DuckDuckGo
* Large models require sufficient RAM
* Internet required for live search

---

## ğŸ”® Possible Extensions

* Streamlit web UI
* PDF + Web hybrid RAG
* Multi-agent search
* Cloud deployment (EC2 / Fly.io)

---

## ğŸ“Œ Key Learning Outcomes

* Built a real-time RAG system
* Used local LLMs without paid APIs
* Applied LangChain LCEL
* Dockerized a GenAI application
* Orchestrated services with Docker Compose

---
